스승님 인공지능 구성요소: 활성화함수, 손실함수, 옵티마이저, 하이퍼 파라미터
오버피팅 방지: 조기종료, 드롭아웃, 가중치 규제, 데이터 증강
정의: 경사하강법, 크로스엔트로피, 옵티마이저, 앙상블, 온라인학습, 서브 샘플링, 전이학습, RNN과 피드포워드 차이, RNN과 LSTM차이, 기울기 소실과 폭주(역전파 시 입력층으로 갈수록 기울기가 점차적으로 작아지는 현상, 기울기가 점차적으로 커지면서 비정상적으로 크게 업데이트 되는것), 원핫인코딩이유

경사하강법: 주어진 손실함수의 최솟값을 찾기위해서 사용되는 최적화 알고리즘
크로스 엔트로피: 두개의 확률분포상의 거리를 측정하는 손실함수
옵티마이저: 손실을 최소화하도록 학습을 수행하는 최적화 알고리즘
앙상블: N개의 동일한 신경망을 독립적으로 훈련시킨 뒤 하나로 합치는 방법으로 성능을 분산시켜 오버피팅을 방지하고 성능을 향상시킨다
온라인 학습: 확률적 경사 하강법으로 전체 훈련 샘플 중 랜덤하게 하나를 뽑아 가중치를 업데이트 시키며 잡음에 취약하다
서브샘플링: 풀링으로 데이터를 요약하여 크기를 줄이는 것으로 매개변수의 수를 줄여 과잉적합을 방지하고 평행이동-불변 즉 물체에 이동에 둔감해지고 가중치가 없는 연산이기에 연산속도가 빠르다. Padding을 주었을 때 stride가 1이면 줄력은 입력과 같아진다.
전이학습: 하나의 문제를 해결하기위한 신경망의 모델과 가중치를 새로운 문제에 적용하는 것이다.
RNN과 피드포워드 차이: 입력이 순서가 있는 데이터/없는 데이터, 입력이 입력데이터와 이전상태/입력데이터, 가변길이입력/고정길이입력
RNN과 LSTM차이: 기울기 소실로 인해 비교적 근거리 상관관계를 고려하는 반면, 장기의존성을 구현하여 긴 시계열 데이터의 상관관계를 고려할 수 있다.

	[Chap 07. MLP와 케라스 라이브러리]
*1. 경사하강법: 주어진 손실함수의 최솟값을 찾기 위해서 사용되는 최적화 알고리즘이다. w=w-lr*grad 옵티마이저 알고리즘이다.
2. 문제는 local minimum의 존재인데, 이를 optimizer(Adam, momentom)이 존재한다. 
    Regression(MSE), Classification(Binary Categorical cross_entropy, Spatial Categorical cross_entropy카테고리컬 크로스엔트로피)등의 손실함수만 알자. Softmax는 활성화함수이다.
3. 정리 (손실함수, 출력층 활성화함수, 출력층 개수) 
	*ㄴ회귀(MSE, X, , 1)
	ㄴ이진분류(binary_crossentropy, sigmoid, 2), 
	ㄴ다중분류(categorical_crossentropy, softmax, class)
4. 몇개의 샘플을 처리한 뒤에 가중치를 업데이트 할 것인가?
	ㄴ풀 배치 학습: 모든 훈련 샘플을 처리한 후에 가중치를 변경_자원을 많이 사용함(시간, 저장)
	ㄴ온라인 학습(확률적 경사 하강법): 훈련 샘플 중에서 무작위로 하나를 골라 학습을 수행_잡음에 취약해짐
	ㄴ미니배치 학습: 훈련 샘플을 작은 배치들로 분리시킨 후에, 하나의 배치가 끝날 때마다 학습을 수행한다._둘의 장점을 갖춰 빠르고 효율적이며 정확함
5. 하이퍼 매개변수: 개발자가 모델에 대해 임의로 결정하는 값(은닉층 수, 유닛의 수(출력층 개수), 학습률, 배치사이즈)_그리드 검색을 사용해서 최적값 찾음
6. 활성화 함수 시그모이드와 계단함수의 차이점은 미분가능성
7. 하이퍼파라미터 튜닝: 라이브러리 기본값, 수동검색, *그리드검색(변경하며 성능 측정), 랜덤 검색

	[Chap 08. 심층 신경망]
1. 은닉층 앞단은 경계선(엣지)같은 저급 특징들을 추출하고, 뒷단은 코너와 같은 고급 특징들을 추출한다.
2. 기존의 sigmoid는 은닉층이 많을 때 아주 큰 양수나 음수가 입력되면, 출력이 포화되어 기울기가 0에 수렴하는 Gradient Vanishing문제가 있다. 이는 가중치와 바이어스 값이 효과적으로 업데이트 되지 못하는 문제이다. 이를 "일부" 해결하기 위해 ReLU(y=max(0,x)를 사용할 수 있다.
3. Softmax함수는 max함수의 소프트한 버전으로, 출력은 전적으로 최대 입력값에 의해 결정된다. 큰수는 더 커지고 작은수는 더 작아지는데 0.0~1.0의 확률값
*4. Cross-Entropy함수는 2개의 확률분포간의 거리를 측정하는 손실함수이다.
**5. 옵티마이저: 손실을 최소화하도록 학습을 수행하는 최적화 알고리즘
6. 가중치 초기화 시 0으로 주면 오차의 역전파가 되지 않고, 동일하게 설정하면 역전파 과정에서 같은 값으로 업데이트 되기에 균형 깨뜨리기가 중요하다. 고로 랜덤값으로 결정한다.
7. 범주형 데이터는 "female"등을 의미하며, 인코딩(정수, 원-핫, 워드임베딩)등이 필요하다.
**8. 언더피팅 방지: 데이터 증강 / 오버피팅 방지: 조기종료(손실증가시 조기종료하여 노이즈 학습 전에 멈춤), 가중치 규제(가중치 감쇠_손실함수에 가중치 절댓값만큼 비용 추가), 드롭아웃(신경망을 부분적으로 생략)
9. DNN 심층신경망과 MLP 멀티레이어퍼셉트론의 차이점은 특징 추출 자체도 학습으로 진행한다는 것이다. 
***10. 앙상블: N개의 동일한 딥러닝 신경망을 독립적으로 학습시킨 후에 마지막에 합친다. 성능을 분산시켜 과적합을 방지하고, 성능을 향상시킨다.**시험**
11. 과잉적합: 지나치게 훈련 데이터에 특화되어 일반화 성능이 떨어지는 것 / 과소적합: 신경망 모델이 너무 단순하거나, 규제가 많아 충분히 훈련되지 않는 것이다.(적절한 패턴을 학습하지 못함)
**12. DNN의 입력값은 -1.0과 1.0사이의 값으로 범위를 제한하는 데이터 정규화를 통해, 신경망이 각 입력 노드에 대한 최적의 매개변수를 빨리 습득하게 한다.
**11. L1 norm(가중치를 0으로), L2 norm(가중치를 0에 가깝게) 가중치 값이 너무 크면 판단이 어려워 지기에 가중치 값을 의도적으로 줄인다. ***시험***
13. 함수형 API는 Input과 Model이용하는거만 기억하면 쉬움
input=Input(shape=(8,))
hidden1=Dense(32, activation='relu')(inputs)
output=Dense(1, activation='relu')(hidden1)
model=Model(inputs=input, outputs=output)

	[Chap 09. 컨볼루션 신경망]
1. 컨볼루션 신경망 유닛사이 연결패턴: 허벨 위젤이 시각피질에서 단순세포와 복합세포를 발견
2. 컨볼루션을 수행하면 영상의 어떤 특징을 뽑아낼 수 있기에 컨볼루션 수행 결과를 특징맵이라고 부른다. 필터를 미리 만드는게 아니라는게 기존 영상처리와의 차이점이다.
*2. 영상에 필터값을 곱하고 더해서 계산. 만약 ReLU까지 적용해야하면 음수인 경우엔 0으로
3. Stride가 k이면 출력은 *1/k로 줄어듬. 
4. 패딩은 가장자리에 커널을 적용하기 위해서 0이나 이웃 픽셀값으로 채우는 것을 의미. 이때 stride가 1이면 입출력 영상의 크기가 동일하다.
*5. 입력 6x6x3(컬러), stride=1, 필터크기 3x3x3, 필터개수 2개일 때 출력 특징 맵은? 4x4x2(마지막은 그냥 커널개수 따라서. 입력영상의 가로세로만 고려)
**6. 서브 샘플링(풀링)은 컨볼루션 연산 수행 직후에 두기도 하는데, 입력 데이터를 요약하여 크기를 줄이는 연산이다. 입력 데이터의 깊이(채널)은 변하지 않는다. 레이어의 크기가 작아져 계산이 빨라져 매개변수가 작아지기에 과잉적합을 방지하고, 물체의 공간이동에 대해 둔감해지게 한다.(이동해도 같은결과)
풀링레이어는 가중치가 없는 일반 연산이다! **웬만해서 크기 구하는거는 직접 그림그려서 해보자!!** **시험_풀링의 장점 3개_빠르고 오버피팅방지  공간이동(영상처리 용어로는 평행이동-불변)**
7. 윈도우 크기만큼 잘라서 주식 훈련 샘플 만들기
def make_sample(data, window): #윈도우 크기
    train=[]
    target=[]
    for i in range(len(data)-window):
        train.append(data[i:i+window])
        target.append(data[i+window])
    return np.array(train), np.array(target)
8. CNN 마지막엔 Dense레이어 2개를 보통 둔다.
9. 1DConv는 sliding_window느낌, 3DConv는 커널이 정육면체.
10. 
def split_sequence(sequence, n_steps): #n_steps는 커널사이즈(1D Conv네). 
    X, y=list(), list()
    for i in range(len(sequence)):
        end_ix=i+n_steps
        if end_ix>len(sequence)-1:
            break
        seq_x, seq_y=sequence[i:end_ix], sequence[end_ix]
        X.append(seq_x)
        y.append(seq_y)
    return array(X), array(y)
*11. Conv신경망에서는 하위 레이어의 유닛(입력)들과 상위 레이어의 유닛(출력)들이 부분적으로 연결되어(커널사이즈만큼만) 있기에 복잡도가 낮아지고 과대적합에 빠지지 않는다.
12. 동물 시각 세포에서 받은 영감은, 시각 피질 뉴런들은 한정된 시야만을 보고, 부분적으로 겹치는 시야를 가진다는 것

	[Chap10. 영상인식]
1. 전이학습: 하나의 문제에 대해 학습한 신경망의 모델과 가중치를, 새로운 문제에 적용하는 것_높은 정확도를 순식간에 얻을 수 있다. **시험**
*2. 전이학습과 파인튜닝의 차이점: 전이학습은 사전모델의 가중치 동결, 분류기를 추가하여 학습시킨다. 파인튜닝은 사전모델의 가중치를 고정하지 않고 일부 혹은 전체를 재학습하는 방법이다. 전이학습은 사전학습된 모델의 일반적인 특징을 사용해 새로운 작업에 빠르게 적용하는 것이 목적이고, 파인튜닝은 새로운 데이터셋에 대해 모델의 성능을 최적화하는 것이 목적이다.
*3. 전이학습이 가능한 이유: 모델의 앞부분에서는 일반적인 특징을 추출하고 뒷부분은 구체적인 특징을 추출하기에 일반적인 특성을 추출하는 데에 사용할 수있다.
4. 가중치 저장 시 저장되는 정보: 모델 아키텍처 구성, 가중치값, 모델컴파일정보, 옵티마이저와 현재상태

	[Chap11. 순환 신경망]
1. 시간적인, 공간적인 순서가 있는 시계열(순차) 데이터에 표준 신경망를 사용하면 멀리 떨어진 과거의 데이터를 잘 기억하지 못하는 단점이 있다. 
2. 피드 포워드 신경망: 데이터가 입력층에서 출력층으로만 전달되는 신경망. 정해진 길이의 입력밖에 받지 못한다는 단점이 있다.(ex 두단어만으로 다음단어)
3. 순환신경망의 기능: 가변길이입력, 장기의존성, 순서정보유지, 시퀀스전체의 파라미터 공유
	*ㄴ이전의 피드 포워드 신경망은 입력에 의해서만 출력이 결정된 반면, 순환신경망은 입력과 이전의 상태를 함께 고려한다.
4. RNN의 한계: 기울기 소실로 인해 장기문맥을 기억하지 못한다.(장기 의존성 문제) 근거리 의존관계만 중시하게 된다. 역전파가 복잡해진다(이전상태고려)
5. LSTM의 구성: 새로운 입력을 조정하는 입력게이트, 어떤 정보를 출력시킬지(다음 은닉상태 결정) 결정하는 출력 게이트, 이전 시점의 입력을 얼마나 기억할 것인지를 결정하는 삭제게이트로 구성된다.
6. RNN과 LSTM의 차이: RNN은 기울기 소실로 인해 비교적 근거리 상관관계만 고려하는 반면, LSTM은 장기의존성을 구현하여 긴 길이의 시계열 데이터를 처리할 수 있다.

[강조]
1. 손실함수: MSE(평균제곱오차), BinaryCrossentropy(이진 정답레이블과 예측레이블간 교차엔트로피 손실계산), CategoricalCrossentropy(여러 레이블 분류, 정답레이블은 원-핫 인코딩), SparseCategoricalCrossentropy(여러 레이블, 정답 레이블 정수인코딩)
2. 하이퍼 파라미터 튜닝이유: 모델의 구조와 기능을 직접 제어하여 최적의 결과를 위한 모델의 성능을 조정하기 위하여.
3. 기울기 폭주: 기울기가 점차 커지며 비정상적으로 커지는 것. 그래디언트 클리핑(임계치), 배치 정규화 등으로 해소
4. 원-핫 인코딩 이유: 데이터의 연관관계나 연속성을 없애 독립적으로 손실을 계산하기 위함
5. 오비피팅방지: 드롭아웃, 가중치 규제, 조기종료, 데이터 증강?
6. 데이터 증강 이유: 소량의 훈련 데이터에서 다양한 훈련 데이터를 뽑아내기 위함. 데이터를 구하기 어려운 경우에 사용.
7. Conv와 Dense의 차이점은 인접한 값들간의 연관관계를 모두 고려한다는 것. MNIST를 flatten하면 위아래 인접픽셀의 관계표현 불가.\
*8. 영상처리와 CNN의 차이점: 필터의 값이 (라플라시안 엣지..) 결정되어 있어 어떤 특징을 추출하는지 알 수 있지만, CNN은 백지상태에서 출발하여 어떤 특징을 추출하는지 알 수 없다. **시험**
9. 패딩이 필요한 이유는 Conv시 가장자리의 픽셀의 값을 제대로 계산할 수 없기 때문이다.

10. 케라스 사용방법: 함수형, 시퀀스, 클래스
11. 풀링 역활 2가지: 매개변수 감소로 인한 계산속도 증가, 평행이동-불변
12. RNN CNN 차이점은 순서가 있는 데이터, 없는 데이터
13. RNN LSTM차이점은 근거리 의존관계를 중시, 장기의존성 구현(짧은 시계열 데이터 처리, 긴 시계열 데이터 처리)














